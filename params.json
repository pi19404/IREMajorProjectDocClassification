{
  "name": "Document Classification",
  "tagline": "",
  "body": "## Aim\r\nGiven a Wikipedia Document our aim is to say the Categories it may belong to, based on a Training data in which each Document is tagged to multiple Categories,  The Categories we considered are the following: \r\n\r\nWiki,Art,Reference,People,Culture,Books,Design,Politics,Technology,Psychology,Interesting,Wikipedia,Research,Religion\r\n,Music,Math,Development,Theory,Philosophy,Article,Language,Science,Programming,History,Software\r\n\r\n## Dataset\r\nWe used Wiki10+ data set from following link:\r\nhttp://nlp.uned.es/social-tagging/wiki10+/\r\n\r\nThe data set contains following two files:\r\n\r\nwiki10+_tag-data.tar.gz  (3,6 MB): Contains all the tag data for the Wikipedia articles.\r\n\r\nwiki10+_documents.tar.bz2  (271 MB): Content for all the Wikipedia articles on the dataset in HTML format. We extracted the text from HTML to run different experiments.\r\n\r\nAs we only consider top 25 documents, we removed those documents who don't have even one of these top 25 categories\r\nThis dataset is made up by 20,764 unique URLs, all of them with their corresponding social tags.   \r\nAll of them are English Wikipedia articles with at least 10 annotations on Delicious. Therefore, the tag information for each of these Wikipedia articles as well as the text content can be found in this dataset.\r\n\r\n\r\n## Approach 1:  LDA or Latent Dirichlet Allocation\r\n\r\nWe can use LDA to classify documents in different tags. We know that LDA divides the given corpus in fixed no. of topics and can also provide which topics are contained in a document and with what probability. For the experiments performed  using LDA, we don’t need to worry about internal implementation of LDA. We used gensim’s implementation of LDA. To use the library, we just need to know few points about input and output format.\r\n\r\n### During Learning phase\r\nINPUT:    \r\nWe provide all the wiki documents in single XML file zipped in bz2 format.\r\n\r\nLEARNT MODEL:  \r\nWord distribution for each topic eg: “topic #0: 0.009*river + 0.008*lake + 0.006*island + 0.005*mountain + \r\n0.004*area + 0.004*park + 0.004*antarctic + 0.004*south + 0.004*mountains + 0.004*dam”\r\n\r\n\r\n### During Testing phase\r\nINPUT:   \r\nWe provide the document to be classified in bag of words form to the learnt model\r\n\r\nOUTPUT:   \r\nTopic distribution for a the text eg: “[(34, 0.023705742561150572), (60, 0.017830310671555303), (62, 0.023999239610385081), (83,0.029439444128473557), (87, 0.028172479800878891), (90, 0.1207424163376625), (116,0.022904510579689157)]”  represents the probabilities of the doc to fall under topics like 34,60,62….\r\n\r\n### Major challenge in classification: \r\nIt seems to be fairly simple to classify a document in different topics as we can see in output of testing phase. But  our aim is to classify the document under different tags like “politics, science” etc. and not under topic numbers. \r\n\r\n\r\n### Possible Solutions\r\n\r\nClearly we need some way to map all the topics learnt by LDA to the most suitable tags. If we are able to do this then we simply test the unknown text against the model learnt by LDA and then report the tag corresponding to the topic given by LDA in output. We tried two different solutions to map topics to the tags:    \r\n\r\n1)  As each topic of LDA is represented by distribution of words. We can create a query by combining those words and find best matched document on tf-idf basis for that query. That particular document must be the best match for that topic. So we can map the topic to tag of best matched document.\r\n\r\n2) We can find probability distribution of topics for all the documents. Represent each document as a topic vector. Now find the closest document or the most similar document for each topic. Map the topic to the tag of that particular document.\r\n\r\n## Approach 1\r\nWe can specify the major steps of to implement this approach as follows:   \r\n\r\n1) Divide the documents in training and test data with 4000 docs in test data.  \r\n\r\n2)On training data run gensim's LDA and save the learnt model. Set the number of topics as 300.   \r\n\r\n3) Save all the topics in a file and convert them to queries.  \r\n### Example topic:\r\n2016-04-06 00:05:52,466 : INFO : topic #299 (0.003): 0.014*insurance + 0.009*scott + 0.007*samurai + 0.007*hipster + 0.006*forecasting + 0.006*fbi + 0.006*imf + \t0.005*skeptical + 0.005*bass + 0.005*hidden\r\n\r\n### Query corresponding to above topic  #299:\r\n 299:insurance scott samurai hipster forecasting fbi imf skeptical bass hidden   \r\n\r\n\r\n4) For each query, retrieve the most relevant document in training set on tf-idf basis and create topic to doc Id mapping.\r\n### Example:\r\n299 : cae3757420fbc4008bbfe492ab0d4cb5    \r\n\r\n\r\n 5)   Create a topic to tag mapping using the docId to tag mapping (already available in tagData.xml) and doc ID to topic mapping created in above step.\r\n\r\n### Example docId to tag from tagData.xml:\r\n cae3757420fbc4008bbfe492ab0d4cb5 : ['wiki', 'en', 'wikipedia,', 'activism', '-‘, 'political', 'poetry', 'free','person', 'music', 'encyclopedia', 'the', 'biography', 'history']    \r\n\r\n###          Example topic to docId:\r\n 299:cae3757420fbc4008bbfe492ab0d4cb5\r\n\r\n###          Example topic to tag:\r\n299:['wiki', 'en', 'wikipedia,', 'activism', '-', 'political', 'poetry', 'free', 'person', 'music‘, 'encyclopedia', 'the','biography', 'history'] \r\n   \r\nNow each topic is mapped to multiple tags. \r\n   \r\n6)   For each of the test documents (from 4000 docs in test data), find out the relevant topics using learnt LDA model. Combine the tags corresponding to them and match them against already available target tags (from tagData.xml) for that particular document.      \r\n         If even one tag is matched, we say that document is correctly classified.\r\n### Example:\r\nTopic distribution returned by LDA for a particular doc:   \r\n\r\n[(34, 0.023705742561150572), (60, 0.017830310671555303), (62, 0.023999239610385081), (83,0.029439444128473557), (87, 0.028172479800878891), (90, 0.1207424163376625), (116,0.022904510579689157), (149, 0.010136256627631658), (155,0.045428499528247894), (162,0.014294122339773195), (192, 0.01315170635603234), (193, 0.055764500858303222), (206,0.015174121956574787), (240, 0.052498569359746373), (243,0.016285345117555323), (247,0.019478047862044864), (255, 0.018193391082926114), (263,0.030209722561452931), (287,0.042405659613804568), (289, 0.055528896333028231),(291,0.030064093091433357)]     \r\n\r\n\r\nTags combined for above topics (from topic to tag mapping created in above step):    \r\n\r\n['money', 'brain', 'web', 'thinking', 'interesting', 'environment', 'teaching', 'web2.0', 'bio', 'finance', 'government', 'food', 'howto', 'geek', 'cool', 'articles', 'school', 'cognitive', 'cognition', 'energy', 'computerscience', '2read', 'culture', 'computer', 'video', 'home', 'todo', 'investment', 'depression', 'psychology', 'wikipedia', 'research', 'health', 'internet', 'medicine', 'electronics', 'tech', 'math', 'business', 'marketing', 'free', 'standard', 'interface', 'article', 'definition', 'anarchism', 'of', 'study', 'economics', 'programming', 'american', 'games', 'advertising', 'social', 'software', 'apple', 'coding', 'maths', 'learning', 'management', 'system', 'quiz', 'pc', 'music', 'memory', 'war', 'nutrition', 'comparison', 'india', 'info', 'science', 'dev', '@wikipedia', 'future', 'behavior', 'design', 'history', '@read', 'mind', 'hardware', 'webdev', 'politics', 'technology‘]    \r\n\r\nTarget tags for this particular doc from tagData.xml:    \r\n\r\n['reference', 'economics', 'wikipedia', 'politics', 'reading', 'resources']   \r\n\r\nAccuracy from this approach: 97% \r\n\r\nProblem with this approach:   \r\n\r\nIf there is any match between our found tags and true tags, then we call it as correctly classified. Probability of such scenario is very high as we have multiple found tags and multiple true tags. So even if we are doing something wrong, chances of getting good accuracy is very high.\r\nAs we are doing tf-idf based matching then there is high chance that the document we get on top is not best match for that particular topic. It can also happen because we are not considering all the representative words of a particular topic to frame the query, we just considered top 10.\r\n\r\n### Approach 2\r\n After analyzing the data we found that only 25 of the tags represent around 19K documents out of 20K. Which simply means that we can eliminate the less frequent tags and docs corresponding to them. Which means we have to divide the corpus among 25 topics at most. Which makes it easier to implement approach 2, as each document can be easily represented in 25 dimensional topic space. We can specify the major steps to implement this approach as follows:\r\n\r\n1) Eliminate the less frequent tags and documents related to them. keep only top 25. Docs left will be around 19K.   \r\n\r\n2) On complete data run gensim's LDA and saved the learnt model. Set number of topics set as 25.  \r\n\r\n3) Save all the topics in a file and convert them to queries as in previous approach.   \r\n\r\n4) Test each of 19K documents against the learnt model and find the topic distribution eg: “42d1d305d10b4b025e01e8237c44c87e:0 0 0 0 0.0242823647949 0 0.037682372871 0 0 0 0.0988683434224 0.0113662521741 0.0157100377468 0 0 0.182273317591 0.205447648234 0 0.0524222798936 0.167240557357 0 0.178899361052 0 0 0” represents the probabilities of the doc with given id in 25 different topics.   \r\n\r\n 5) Using above distribution find out the most relevant document for a particular topic and map it to the tag of that document. It gives the similar topic to tag mapping as in previous approach.   \r\n\r\n6) Now many topics must have matched to more than one tag. Manually check which tag is best suited for that particular topic depending on words contained in the topic. As a result we have each topic mapped to at most one tag.   \r\n\r\n7) Now perform the testing as done in step 6 of previous approach but on all 19K docs.  \r\n\r\n\r\nAccuracy from this approach: 88% \r\n\r\n\r\nProblem with this approach:  \r\n\r\nMapping topics to tags manually is an issue. We can’t always find out the best suited tag just by seeing the topic words.  \r\n Sometimes tags don’t reflect anything eg: ‘wikipedia’, ‘wiki’, ‘reference’ create problem.   \r\n\r\n\r\nModification:  \r\n\r\nPerformed the above experiment again but just with meaningful tags i.e. no tag like  ‘wikipedia’, ‘wiki’,  \r\n‘reference’  etc. After eliminating these documents left were 17K.  But the approach posed another issue:   \r\n\r\n1) There are similar tags which can represent a topic at the same time eg: [research, science], [web, internet], [programming, math], [literature, language].  \r\n\r\nIf we keep all such similar tags then accuracy is : 80% but if we strictly keep just one tag then accuracy drops  to 65%.\r\nReason for the drop is possibly manual work. We can’t surely say which tag should be kept when both tags are \r\nsame.    \r\n\r\nConclusion: 2ND approach is better as there is very less chance of false positives and accuracy is also not bad considering just ~19K documents for learning.  \r\nNote:Rest of the methods are ML classification based approaches with vectorial input\r\n\r\n  \r\n## Approach 2: tf-idf feature vectors\r\n\r\nTf-idf reflects how important a word is to a document in a collection or corpus. It's value increases proportionally to the number of times a word appears in the document   \r\nThe choice of idf weighting scheme is Inverse document frequency      \r\nThe choice of tf weighting scheme is log normalization\t  \r\nAs Each vector size could be at max the size of the Vocabulary, we only taken top 500 words (most rarest) , restricting   maximum number of features to 500.  \r\nTime taken to build the matrix is around 5 Minutes.  \r\nNote that it Generates Relative Vectors , that is vector generation of one document depends on vector generation of other   documents, therefore given  one new document, Vectors for all documents have to be updated  \r\nPreprocessing includes Case Folding, Stemming and stopword removal done (sklearn has inbuilt stop word removal)  \r\n\r\n\r\n## Approach 3: Doc2Vec vectors\r\n\r\nWe Used Distributed Memory Algorithm version of Doc2Vec, It has the Potential to overcome many weaknesses of bag-of-words models.The vector representations are learned to predict the surrounding words in contexts sampled from the document.\r\nFor Example, For our 20,000 documents, Training takes around 2 hours, and generates 4GB model which stores the mapping between each document and a 300 size (size chosen arbitrarily) vectors\r\nin other words, training the model itself is the document feature extraction\r\nBecause of which it's an even bad Relative feature extraction, even for one more test document, the entire training of 2 hours  needs to be run again.\r\nas part of preprocessing : Case Folding, Stemming and stopword removal was done\r\n\r\n## Approach 4: Word2Vec vectors\r\n\r\nWord2Vec is a Word Embedding which is based on Co-Occurrence matrix or  explicit representation in terms of the context in which words appear. We Used Pre-Trained 4.6 GB model trained on English News Corpus as we had a corpus relatively, training on it wouldn't give us that good vectors.\r\nWe did Preprocessing to Remove Stop Words, Words not existing in the model removed as no vector available, No stemming or case folding as that's how the model was originally trained i.e \r\nvector(“computer”) ≠ vector(“Computer”) ≠ vector(“Compute”) ≠  vector(“compute”)\r\nFor Document Vector , Average was done over the 100 size vectors of the words left in the document after pre-processing.\r\nEven though Training the model is already done, Extracting vectors around 4000 times for  each of 20000 documents takes a lot of time around  5 hours.\r\nBut, Advantage is that Word2Vec Generates Absolute Vectors, that is the vector generation of one document is independant of vector generation of other documents, Hence converting one single document to vector is very fast.\r\n\r\n### Classification,Results and Comparison\r\n\r\nTesting And Training\r\nBefore doing Any Feature Extraction Documents were randomly shuffled and 5000 documents randomly taken out as Testing, rest of the around 15,000 as Training.\r\nAssertion was made that every category in testing at least occurs once in training.\r\nFor all feature extraction same sets of documents used, no shuffling again so that we can compare models . that is all Feature extraction models were trained against same training data and tested against same testing data.\r\nClassifier Used : SVM with SGD training\r\nWhy? \r\nAs we have separate Independent Classifier for each label (multi-class training)  and Vectors are up to 100 to 1000 in length. Other Classifiers take too much time where Stochastic Gradient Design is very fast. Therefore we went with this classifier.\r\nWe used Sklearn’s Implementation:\r\nhttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\r\nAccuracies\r\nNote that our data is quite imbalanced, that is the green line “Maxclass” Represents the Accuracy when all vectors in testing are assigned the maximum occurring label from Training, As evident it is , it's value is almost always above 80%  \r\n\r\n1) The Topic Distribution Vectors generated from LDA seem to be as bad as Maxclass assignment, (hence they overlap in the figure) when applied to the classifier, However LDA training might be the fastest of all. \r\n\r\n2)It's difficult to compare doc2vec and tf-idf but doc2vec performs better than word2vec, It's also faster than word2vec when it comes to generating 20,000 vectors \r\n\r\n3)Word2Vec didn't perform so good and also took quite a time for vector extraction of all documents, the only advantage is that it’s feature extraction of one document doesn't affect other vectors, hence less time for 1 test document\r\n\r\n4) tf-idf  heavily depends on the number of feature words we consider , In this experiment we took 500 words, but by increasing the size, accuracy may increase but will take more time and space  to store too.\r\n\r\n5) LDA performs good if we look at the accuracy but we can’t guarantee good accuracy always as there can be error in manually choosing best tag for topics.  \r\n\r\n\r\n  \r\n\r\n\r\n\r\n## Authors and Contributors\r\nMohit Sharma, Vijjini Anvesh Rao, T.V.Ravi Teja\t      \r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}